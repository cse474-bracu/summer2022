<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Data | CSE474</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Data" /><meta name="author" content="Meem Arafat Manab" /><meta property="og:locale" content="en_US" /><meta name="description" content="Simulation methods, model building, random number generator, statistical analysis of results, validation and verification techniques." /><meta property="og:description" content="Simulation methods, model building, random number generator, statistical analysis of results, validation and verification techniques." /><link rel="canonical" href="http://localhost:4000/lectures/data/" /><meta property="og:url" content="http://localhost:4000/lectures/data/" /><meta property="og:site_name" content="CSE474" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Data" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Meem Arafat Manab"},"headline":"Data","description":"Simulation methods, model building, random number generator, statistical analysis of results, validation and verification techniques.","url":"http://localhost:4000/lectures/data/","@type":"WebPage","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/bracu_logo.png"},"name":"Meem Arafat Manab"},"@context":"https://schema.org"}</script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, } }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="http://localhost:4000/" class="site-title lh-tight"><div class="site-logo"></div></a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a><li class="nav-list-item active"><a href="http://localhost:4000/lectures/" class="nav-list-link">Lectures</a><li class="nav-list-item"><a href="http://localhost:4000/paper-reviews/" class="nav-list-link">Paper Reviews</a><li class="nav-list-item"><a href="http://localhost:4000/projects/" class="nav-list-link">Final Projects</a><li class="nav-list-item"><a href="http://localhost:4000/resources/" class="nav-list-link">Resources</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CSE474" aria-label="Search CSE474" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/lectures/">Lectures</a><li class="breadcrumb-nav-list-item"><span>Data</span></ol></nav><div id="main-content" class="main-content" role="main"><p>So far, we’ve talked about the behavior (capabilities and harms) of large language models. Now, we peel open the first layer of the onion and start discussing how these models are constructed. The starting point of any machine learning approach is <strong>training data</strong>, so this is where we’ll start.</p><p><em>Aside</em>: Normally in machine learning, the training data and the test (evaluation) data are similar or at least of the same type. But for large language models, the training data is just “raw text”.</p><p>In the rest of the lecture, we’ll talk about:</p><ol><li><a href="#data-behind-large-language-models">Data behind large language models</a><li><a href="#documentation-of-datasets">Documentation of datasets</a><li><a href="#data-ecosystems">Data ecosystems</a></ol><h2 id="data-behind-large-language-models"> <a href="#data-behind-large-language-models" class="anchor-heading" aria-labelledby="data-behind-large-language-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Data behind large language models</h2><p>Recall that large language models are trained on “raw text”. To be highly capable (e.g., have linguistic and world knowledge), this text should span a <strong>broad</strong> range of domains, genres, languages, etc.</p><p>A natural place (but not the only place) to look for such text is the <strong>web</strong>, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (<a href="https://www.google.com/search/howsearchworks/how-search-works/organizing-information/">reference</a>). The actual web is likely even larger, and the <a href="https://en.wikipedia.org/wiki/Deep_web">Deep Web</a> is even larger than that.</p><p>It is worth noting that <strong>private datasets</strong> that reside in big companies are even larger than what’s available publicly. For example, <a href="https://www.forbes.com/sites/bernardmarr/2017/01/23/really-big-data-at-walmart-real-time-insights-from-their-40-petabyte-data-cloud">WalMart</a> generates 2.5 petabytes of data each hour!</p><p><strong>Common Crawl</strong> is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of <a href="https://en.wikipedia.org/wiki/Common_Crawl">Common Crawl</a> has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.</p><p><strong>Representation</strong>. Despite the richness of web data, it has been noted in <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">Bender et al, 2021</a> that:</p><ul><li>Despite the size, large-scale data still has <strong>uneven representation</strong> over the population.<li>Internet data overrepresents younger users from developed countries.<li>GPT-2’s training data is based on Reddit, which according to Pew Internet Research’s 2016 survey, 67% of Reddit users in the US are men, 64% between ages 18 and 29.<li>8.8-15% of Wikipedians are female.<li>Harassment on Internet could turn away certain people (trans, queer, neurodivergent people).<li>Filtering “bad words” could further marginalize certain populations (e.g., LGBT+).</ul><p>Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models.</p><h3 id="webtext-and-openwebtext"> <a href="#webtext-and-openwebtext" class="anchor-heading" aria-labelledby="webtext-and-openwebtext"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> WebText and OpenWebText</h3><p><strong>WebText</strong>. The WebText dataset was used to train GPT-2.</p><ul><li>Goal: obtain <strong>diverse</strong> but <strong>high-quality</strong> dataset.<li>Previous work:<ul><li>Datasets were trained on news, Wikipedia, or fiction.<li>Common Crawl contains a lot of junk (gibberish, boilerplate text).<li><a href="https://arxiv.org/pdf/1806.02847.pdf">Trinh &amp; Le, 2018</a> selected a tiny subset of Common Crawl based on n-gram overlap with the target task.</ul><li>Process for creating WebText:<ul><li>Scraped all outbound links that received at least 3 karma (upvotes).<li>Filtered out Wikipedia to be able to evaluate on Wikipedia-based benchmarks.<li>End result is 40 GB of text.</ul></ul><p><strong>OpenWebText</strong>. WebText was not released by OpenAI, but it was replicated (in spirit) by the <a href="https://skylion007.github.io/OpenWebTextCorpus/">OpenWebText</a> dataset.</p><ul><li>Extracted all the URLs from the <a href="https://files.pushshift.io/reddit/submissions/">Reddit submissions dataset</a>.<li>Used Facebook’s <a href="https://github.com/facebookresearch/fastText">fastText</a> to filter out non-English.<li>Removed near duplicates.<li>End result is 38 GB of text.</ul><p><strong>Toxicity analysis</strong>. <a href="https://arxiv.org/pdf/2009.11462.pdf">Gehman et al. 2020</a>, the RealToxicityPrompts paper, analyzed these two datasets and found:</p><ul><li>2.1% of OpenWebText has toxicity score &gt;= 50%<li>4.3% of WebText (from OpenAI) has toxicity score &gt;= 50%<li>News reliability correlates negatively with toxicity (Spearman \(\rho = -0.35\))<li>3% of OpenWebText comes from <a href="https://en.wikipedia.org/wiki/Controversial_Reddit_communities">banned or quarantined subreddits</a>, e.g., /r/The_Donald and /r/WhiteRights</ul><h3 id="colossal-clean-crawled-corpus"> <a href="#colossal-clean-crawled-corpus" class="anchor-heading" aria-labelledby="colossal-clean-crawled-corpus"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Colossal Clean Crawled Corpus</h3><p>The Colossal Clean Crawled Corpus (<a href="https://www.tensorflow.org/datasets/catalog/c4">C4</a>) is a larger was created to train the T5 model.</p><ul><li>Started with April 2019 snapshot of Common Crawl (1.4 trillion tokens)<li>Removed <a href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en">“bad words”</a><li>Removed code (“{“)<li>langdetect to filter out non-English text<li>Resulted in 806 GB of text (156 billion tokens)</ul><p><strong>Analysis</strong>. <a href="https://arxiv.org/pdf/2104.08758.pdf">Dodge et al. 2021</a> performed a thorough analysis of the C4 dataset.</p><p>Documentation levels:</p><ul><li>Metadata: provenance, utterance data<li>Included data: machine or human authored, social biases, data contamination<li>Excluded data: medical or health data, demographic identities</ul><p>Note: <a href="https://arxiv.org/pdf/1910.10683.pdf">Raffel et al. 2020</a> only provided scripts to recreate; cost thousands of dollars just to run these scripts.</p><p><img src="../images/c4-domains.png" alt="C4 domains" /></p><ul><li>A surprising amount of data from patents.google.com<li>65% pages in the Internet Archive; out of those, 92% pages written in the last decade<li>51.3% pages are hosted in the United States; fewer from India even though lots of English speakers there<li>Some text from patents.google.com are automatically created, and thus have systematic errors:<ul><li>Filed in a foreign country’s official language (e.g., Japanese) is automatically translated into English<li>Automatically generated from optical character recognition (OCR)</ul></ul><p><strong>Benchmark data contamination</strong>.</p><ul><li>When we are evaluating the capabilities of large language models using benchmark data (e.g., question-answer pairs), it makes a difference whether the benchmark data appears in the training data of the language model. If so, then the benchmark performance will be <strong>biased</strong> up.<li>Normally, in machine learning, data hygiene (keeping the training data separate from the test) is relatively easy, but in the case of large language models, both the training data and benchmark data are derived from the Internet, it can be difficult to a priori guarantee their separation.</ul><p>Example from the <a href="https://huggingface.co/datasets/xsum">XSum</a> summarization dataset:</p><blockquote><p><strong>Input</strong>: <em>The 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed youth academy director in 2000 and has been director of football since 2003. A West Brom statement said: “He played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012.</em><br /> <strong>Output</strong>: <em>West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.</em><br /></p></blockquote><p>There are two types of contamination:</p><ul><li><strong>Input-and-output contamination</strong>: both the input and output appear in the training data. Varies from 1.87% to 24.88% (XSum is 15.49%).<li><strong>Input contamination</strong>: the input appears in the training data. Varies from 1.8% to 53.6% (QNLI, which is derived from Wikipedia).</ul><p>Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage).</p><p>The dataset could also be responsible for various harms:</p><ul><li><strong>Representational harms</strong><ul><li>They look at co-occurrence with ethnicity terms (e.g., <em>Jewish</em>) and <a href="https://arxiv.org/pdf/1606.02820.pdf">sentiment-bearing words</a> (e.g., <em>successful</em>).<li><em>Jewish</em> has 73.2% positive sentiment, <em>Arab</em> has 65.7% positive (7.5% difference).<li>Variation across sites (New York Times had a 4.5% difference, Al Jazeera had 0% difference).</ul><li><strong>Allocational harms</strong><ul><li>Recall C4 is a filtered version of Common Crawl (only about 10%).<li>Mentions of sexual orientations (e.g., <em>lesbian</em>, <em>gay</em>) more likely to be filtered out; of those filtered out, non-trivial fraction are non-offensive (e.g., 22% and 36%).<li>Certain dialects are more likely to be filtered (AAE: 42%, Hispanic-aligned English: 32%) than others (White American English: 6.2%)</ul></ul><p><img src="../images/c4-excluded.png" alt="Words in documents that get filtered out of C4" /></p><h3 id="gpt-3-dataset"> <a href="#gpt-3-dataset" class="anchor-heading" aria-labelledby="gpt-3-dataset"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> GPT-3 dataset</h3><p><img src="../images/gpt3-dataset.png" alt="GPT-3 dataset" /></p><ol><li>Selected subset of Common Crawl that’s <strong>similar to a reference dataset</strong> (WebText).<ul><li>Downloaded 41 shards of Common Crawl (2016-2019).<li>Trained a binary classifier to predict WebText versus Common Crawl.<li>Sampled (kept) a document with higher probability if classifier deems it more similar to WebText.</ul><li>Performed <strong>fuzzy deduplication</strong> (detect 13-gram overlap, remove window or documents if occurred in &lt;10 training documents), removing data from benchmark datasets.<li>Expanded the diversity of the <strong>data sources</strong> (WebText2, Books1, Books2, Wikipedia).<li>During training, Common Crawl is downsampled (Common Crawl is 82% of the dataset, but contributes only 60%).</ol><h3 id="the-pile"> <a href="#the-pile" class="anchor-heading" aria-labelledby="the-pile"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Pile</h3><ul><li>While a web crawl is a natural place to look for broad data, it’s not the only strategy, and GPT-3 already hinted that it might be productive to look at other sources of higher quality.<li>EleutherAI (a nonprofit organization committed to building open language models), pushed this idea even farther.<li>They released <a href="https://arxiv.org/pdf/2101.00027.pdf">The Pile</a>, a dataset for language modeling, where the key idea is to source it from smaller high-quality sources (academic + professional sources).</ul><p><strong>Data composition</strong>.</p><ul><li>825 GB English text<li>22 high-quality datasets</ul><p><img src="../images/the-pile.png" alt="The Pile" /></p><p>Compare:</p><ul><li>GPT-2Pile (1.5B parameters) trained on The Pile<li>GPT-3 (175B parameters) trained on GPT-3’s dataset.<li>Normalize so that the difference for OpenWebText2 is 0.</ul><p><img src="../images/the-pile-gpt2-gpt3.png" alt="GPT-2 trained on the Pile compared to GPT-3" /></p><p>Takeaway: The Pile contains a lot of information that’s not well covered by GPT-3’s dataset.</p><p>They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.</p><h3 id="summary"> <a href="#summary" class="anchor-heading" aria-labelledby="summary"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Summary</h3><ul><li>The total amount of data out there (web, private data) is massive.<li>Training on “all of it” (even Common Crawl) doesn’t work well (not effective use of compute).<li>Filtering / curation (OpenWebText, C4, GPT-3 dataset) is needed, but can result in biases.<li>Curating non-web high-quality datasets is promising (The Pile).<li>Important to carefully document and inspect these datasets.</ul><h2 id="documentation-for-datasets"> <a href="#documentation-for-datasets" class="anchor-heading" aria-labelledby="documentation-for-datasets"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Documentation for datasets</h2><p>We now step back from the specifics of language modeling datasets and discuss general principles around data.</p><ul><li>It has been long noted that documentation is important, but within the machine learning community, it has been a fairly ad-hoc process.<li>Examples from other fields:<ul><li><strong>Electronics industry</strong> has a well-established protocol where every component has a datasheet with operating characteristics, test results, recommended and usage.<li><strong>Nutrition labels</strong>: The FDA mandates that food be labeled with their nutrition content.</ul><li>Datasheets for datasets (<a href="https://arxiv.org/pdf/1803.09010.pdf">Gebru et al., 2018</a>) is an influential paper that provides community norms around documentation.<li>Data statements (<a href="https://aclanthology.org/Q18-1041.pdf">Bender &amp; Friedman, 2018</a>) is related framework that is more tailored to language datasets.<li>The emphasis is on <strong>transparency</strong>.</ul><p>Two purposes:</p><ol><li><strong>Dataset creators</strong>: reflect on decisions, potential harms (e.g., social biases) when creating the dataset.<li><strong>Dataset consumers</strong>: know when the dataset can and can’t be used.</ol><p><strong>Dataset lifecycle</strong> (a sample of the questions from each category are provided below):</p><ul><li>Motivation<ul><li>For what purpose was the dataset created?<li>Who created this dataset?<li>Who funded the creation of the dataset?</ul><li>Composition<ul><li>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?<li>Is any information missing from individual instances?<li>Does the dataset contain data that might be considered confidential?</ul><li>Collection process<ul><li>How was the data associated with each instance acquired?<li>Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?<li>Were any ethical review processes conducted (e.g., by an institutional review board)?</ul><li>Preprocessing/cleaning/labeling<ul><li>Was any preprocessing/cleaning/labeling of the data done?<li>Is the software that was used to preprocess/clean/label the data available?</ul><li>Uses<ul><li>Has the dataset been used for any tasks already?<li>Are there tasks for which the dataset should not be used?</ul><li>Distribution<ul><li>How will the dataset will be distributed?<li>Have any third parties imposed IP-based or other restrictions on the data associated with the instances?</ul><li>Maintenance<ul><li>Who will be supporting/hosting/maintaining the dataset?<li>Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?</ul></ul><p><strong>Data statements</strong>. The <a href="https://aclanthology.org/Q18-1041.pdf">data statements</a> work is specialized to NLP datasets, and covers other aspects:</p><ul><li>Curation rationale (what’s included?)<li>Language variety (<a href="https://tools.ietf.org/rfc/bcp/bcp47.txt">schema</a>)<li>Speaker demographic (age, gender, race/ethnicity, etc.)<li>Annotator demographic (age, gender, race/ethnicity, etc.)</ul><p>As an example, let’s look at the <a href="https://arxiv.org/pdf/2201.07311.pdf">datasheet for The Pile</a>.</p><h2 id="data-ecosystems"> <a href="#data-ecosystems" class="anchor-heading" aria-labelledby="data-ecosystems"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Data ecosystems</h2><p>So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles.</p><p><strong>Data management</strong>: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant in <strong>industry</strong>.</p><ul><li>Some issues are discussed in the <a href="https://crfm.stanford.edu/assets/report.pdf#data">data section of the foundation models report</a>.<li><a href="https://en.wikipedia.org/wiki/Data_governance">Data governance</a> talks about how an organization can create data, maintain its quality and security.<li>The BigScience project (initiated by Hugging Face) aims to collect a large multilingual dataset as well as train a large language model. The <a href="https://www.youtube.com/watch?v=NL1_kMOkHm8">BigScience data governance working group</a> has been developing a framework to responsibly curate quality data sources, in contrast to the indiscriminate scraping of the web.</ul><p><img src="../images/bigscience-data-governance.png" alt="BigScience data governance" /></p><p><strong>Data dignity</strong>. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.</p><ul><li>People create data.<li>Because people live in social environments, data also is a property not of individuals, but of groups of people. Examples: emails, genetic data.<li>Individually, data does not have value, but collectively, it has a lot of value.<li>Related: <a href="https://arxiv.org/pdf/1904.02868.pdf">Data Shapley</a> is a framework for ascribing value to a given data point in the context of machine learning.<li>Status quo: people give away their data for free, and big corporations derive tons of value and power from it.<ul><li>Example: Alice and Bob are both writers. Alice provide examples of writing for free. This can be used to train a language model that can replace Bob.</ul><li>Think about data as <strong>labor</strong> rather than property rights.<li>Data privacy works on the individual level, and doesn’t work.<li>Proposal: <strong>data coalitions</strong>, which are intermediate organizations that represent between data producers and data buyers (think about collective bargaining).<li>Read <a href="https://www.radicalxchange.org/media/papers/data-freedom-act.pdf">this article</a> for more details.</ul><h2 id="further-reading"> <a href="#further-reading" class="anchor-heading" aria-labelledby="further-reading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Further reading</h2><p>Documentation for datasets:</p><ul><li><a href="https://arxiv.org/pdf/1803.09010.pdf">Datasheets for datasets</a>. <em>Timnit Gebru, Jamie H. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, Kate Crawford</em>. Communications of the ACM 2018.<li><a href="https://aclanthology.org/Q18-1041.pdf">Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science</a>. <em>Emily M. Bender and Batya Friedman</em>. ACL 2018.<li><a href="https://arxiv.org/pdf/1810.03993.pdf">Model Cards for Model Reporting</a>. <em>Margaret Mitchell, Simone Wu, Andrew Zaldivar, P. Barnes, Lucy Vasserman, B. Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru</em>. FAT 2018.</ul><p>Datasets:</p><ul><li><a href="http://commoncrawl.org/">CommonCrawl</a><li><a href="https://skylion007.github.io/OpenWebTextCorpus/">OpenWebText</a> Similar to WebText, used to train GPT-2.<li><a href="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>. <em>Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, Peter J. Liu</em>. J. Mach. Learn. Res. 2019. Introduces <strong>Clossal Clean Crawled Corpus (C4)</strong> and the T5 model.<li><a href="https://arxiv.org/pdf/1911.00359.pdf">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data</a>. <em>Guillaume Wenzek, Marie-Anne Lachaux, A. Conneau, Vishrav Chaudhary, Francisco Guzm’an, Armand Joulin, Edouard Grave</em>. LREC 2019. Introduces <strong>CCNet</strong>.<li><a href="https://arxiv.org/pdf/2101.00027.pdf">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</a>. <em>Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy</em>. 2020. Introduces <strong>The Pile</strong>. Introduces <strong>The Pile</strong>, used to train GPT-J.<li><a href="https://arxiv.org/pdf/1911.02116.pdf">Unsupervised Cross-lingual Representation Learning at Scale</a>. <em>A. Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov</em>. ACL 2019. Introduces cleaned versions of CommonCrawl corpus on 100 datasets, used to train XLM-R.</ul><p>Analysis of datasets:</p><ul><li><a href="https://arxiv.org/pdf/2104.08758.pdf">Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus</a>. <em>Jesse Dodge, Ana Marasović, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner</em>. EMNLP 2021.<li><a href="https://arxiv.org/pdf/2103.12028.pdf">Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets</a>. <em>Isaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, D. Esch, Nasanbayar Ulzii-Orshikh, A. Tapo, Nishant Subramani, A. Sokolov, Claytone Sikasote, Monang Setyawan, S. Sarin, Sokhar Samb, B. Sagot, Clara Rivera, Annette Rios Gonzales, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Rubungo Andre Niyongabo, Toan Q. Nguyen, Mathias Muller, A. Muller, S. Muhammad, N. Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, M. Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, N. D. Silva, Sakine cCabuk Balli, Stella Rose Biderman, A. Battisti, Ahmed Baruwa, Ankur Bapna, P. Baljekar, Israel Abebe Azime, A. Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, Mofetoluwa Adeyemi</em>. 2021.</ul><p>Filtering datasets:</p><ul><li><a href="https://arxiv.org/pdf/2109.00698.pdf">An Empirical Exploration in Quality Filtering of Text Data</a>. <em>Leo Gao</em>. 2021.<li><a href="https://arxiv.org/pdf/2107.06499.pdf">Deduplicating Training Data Makes Language Models Better</a>. <em>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, D. Eck, Chris Callison-Burch, Nicholas Carlini</em>. 2021.</ul><p>Data ecosystems:</p><ul><li><a href="https://crfm.stanford.edu/assets/report.pdf#data">Foundation models report (data section)</a><li><a href="https://www.youtube.com/watch?v=NL1_kMOkHm8">BigScience data governance working group</a><li><a href="https://arxiv.org/pdf/1904.02868.pdf">Data Shapley: Equitable Valuation of Data for Machine Learning</a>. <em>Amirata Ghorbani, James Y. Zou</em>. ICML 2019.<li><a href="https://www.radicalxchange.org/media/papers/data-freedom-act.pdf">Data Freedom Act</a></ul><hr><footer> <!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Untitled</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css"><link rel="stylesheet" href="assets/css/style.css"><body><div class="footer-clean"><footer><div class="container"><div class="row justify-content-center"><p class="copyright">Made and Compiled by <a href="https://www.linkedin.com/in/joyanta0/">Joyanta.</a></p><p class="copyright">Department of Computer Science and Engineering, School of Data and Sciences BRAC University © 2023</p></div></div></div></footer></div><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.bundle.min.js"></script></footer></div></div><div class="search-overlay"></div></div>
