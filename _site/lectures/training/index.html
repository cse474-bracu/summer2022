<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Training | CSE474</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Training" /><meta name="author" content="Meem Arafat Manab" /><meta property="og:locale" content="en_US" /><meta name="description" content="Simulation methods, model building, random number generator, statistical analysis of results, validation and verification techniques." /><meta property="og:description" content="Simulation methods, model building, random number generator, statistical analysis of results, validation and verification techniques." /><link rel="canonical" href="http://localhost:4000/lectures/training/" /><meta property="og:url" content="http://localhost:4000/lectures/training/" /><meta property="og:site_name" content="CSE474" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Training" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Meem Arafat Manab"},"headline":"Training","description":"Simulation methods, model building, random number generator, statistical analysis of results, validation and verification techniques.","url":"http://localhost:4000/lectures/training/","@type":"WebPage","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/bracu_logo.png"},"name":"Meem Arafat Manab"},"@context":"https://schema.org"}</script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, } }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="http://localhost:4000/" class="site-title lh-tight"><div class="site-logo"></div></a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Home</a><li class="nav-list-item active"><a href="http://localhost:4000/lectures/" class="nav-list-link">Lectures</a><li class="nav-list-item"><a href="http://localhost:4000/paper-reviews/" class="nav-list-link">Paper Reviews</a><li class="nav-list-item"><a href="http://localhost:4000/projects/" class="nav-list-link">Final Projects</a><li class="nav-list-item"><a href="http://localhost:4000/resources/" class="nav-list-link">Resources</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search CSE474" aria-label="Search CSE474" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/lectures/">Lectures</a><li class="breadcrumb-nav-list-item"><span>Training</span></ol></nav><div id="main-content" class="main-content" role="main"><p>\(\newcommand{\sV}{\mathcal{V}} \newcommand{\sO}{\mathcal{O}} \newcommand{\sD}{\mathcal{D}} \newcommand{\sN}{\mathcal{N}} \newcommand{\R}{\mathbb{R}} \newcommand{\E}{\mathbb{E}} \newcommand{\x}{x_{1:L}} \newcommand{\tx}{\tilde x_{1:L}} \newcommand{\nl}[1]{\textsf{#1}} \newcommand{\softmax}{\text{softmax}} \newcommand{\TransformerBlock}{\text{TransformerBlock}} \newcommand{\EmbedTokenWithPosition}{\text{EmbedTokenWithPosition}} \newcommand{\SentenceEmbedding}{\text{SentenceEmbedding}} \newcommand{\BERT}{\text{BERT}} \newcommand{\MASK}{\nl{[MASK]}} \newcommand{\SEP}{\nl{[SEP]}} \newcommand{\CLS}{\nl{[CLS]}} \newcommand{\generate}[1]{\stackrel{#1}{\rightsquigarrow}} \newcommand{\embed}{\stackrel{\phi}{\Rightarrow}}\) Last lecture, we talked about the <a href="../lectures/modeling">model architecture</a> for large language models (e.g., the Transformer). In this lecture, we will discuss how to train large language models.</p><ol><li><a href="#objective-functions">Objective functions</a><li><a href="#optimization-algorithms">Optimization algorithms</a></ol><h2 id="objective-functions"> <a href="#objective-functions" class="anchor-heading" aria-labelledby="objective-functions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Objective functions</h2><p>We will consider objective functions for the three types of language models:</p><ol><li>Decoder-only (e.g., GPT-3): compute unidirectional contextual embeddings, generate one token at a time<li>Encoder-only (e.g., BERT): compute bidirectional contextual embeddings<li>Encoder-decoder (e.g., T5): encode input, decode output</ol><p>We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):</p>\[\phi : \sV^L \to \R^{d \times L}.\] \[[\nl{the}, \nl{mouse}, \nl{ate}, \nl{the}, \nl{cheese}] \embed \left[\binom{1}{0.1}, \binom{0}{1}, \binom{1}{1}, \binom{1}{-0.1}, \binom{0}{-1} \right].\]<h3 id="decoder-only-models"> <a href="#decoder-only-models" class="anchor-heading" aria-labelledby="decoder-only-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Decoder-only models</h3><p>Recall that an autoregressive language model defines a conditional distribution:</p>\[p(x_i \mid x_{1:i-1}).\]<p>We define it as follows:</p><ul><li>Map \(x_{1:i-1}\) to contextual embeddings \(\phi(x_{1:i-1})\).<li>Apply an embedding matrix \(E \in \R^{V \times d}\) to obtain scores for each token \(E \phi(x_{1:i-1})_{i-1}\).<li>Exponentiate and normalize it to produce the distribution over \(x_i\).</ul><p>Succinctly:</p>\[p(x_{i+1} \mid x_{1:i}) = \softmax(E \phi(x_{1:i})_i).\]<p><strong>Maximum likelihood</strong>. Let \(\theta\) be all the parameters of large language models.</p><p>Let \(\sD\) be the <strong>training data</strong> consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:</p>\[\sO(\theta) = \sum_{\x \in \sD} -\log p_\theta(\x) = \sum_{\x \in \sD} \sum_{i=1}^L -\log p_\theta(x_i \mid x_{1:i-1}).\]<p>There’s more to say about how to efficiently optimize this function, but that’s all there is for the objective.</p><h3 id="encoder-only-models"> <a href="#encoder-only-models" class="anchor-heading" aria-labelledby="encoder-only-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Encoder-only models</h3><p><strong>Unidirectional to bidirectional</strong>. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don’t need to generate.</p><p><strong>BERT</strong>. We will first present the <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a> objective function, which contains two terms:</p><ol><li>Masked language modeling<li>Next sentence prediction</ol><p>Take the example sequence for natural language inference (predict entailment, contradiction, or neutral):</p>\[\x = [\CLS, \nl{all}, \nl{animals}, \nl{breathe}, \SEP, \nl{cats}, \nl{breathe}].\]<p>There are two special tokens:</p><ul><li>\(\CLS\): contains the embedding used to drive classification tasks<li>\(\SEP\): used to tell the model where the first (e.g., premise) versus second sequence (e.g., hypothesis) are.</ul><p>Using our notation from the previous lecture, the BERT model is defined as:</p>\[\BERT(\x) = \TransformerBlock^{24}(\EmbedTokenWithPosition(\x) + \SentenceEmbedding(\x)) \in \R^{d \times L},\]<p>where \(\SentenceEmbedding(\x)\) returns one of 2 vectors depending on the sequence:</p><ul><li>\(e_A \in \R^d\) for tokens <strong>left</strong> of \(\SEP\), and<li>\(e_B \in \R^d\) for tokens <strong>right</strong> of \(\SEP\).</ul><p><img src="../images/bert.png" alt="BERT model" /></p><p>BERT-large has \(n_\text{heads} = 16\) attention heads, and a \(d_\text{model} = 1024\) dimensional model, resulting in 355M parameters.</p><p><strong>Masked language modeling</strong>. The basic idea of the masked language model is to train on the prediction problem:</p>\[[\nl{the}, \MASK, \nl{ate}, \MASK, \nl{cheese}] \Rightarrow [\nl{the}, \nl{mouse}, \nl{ate}, \nl{the}, \nl{cheese}].\]<p>More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \(\tx\) and try to reconstruct the original \(\x\).</p>\[\tx \Rightarrow \x.\]<p><strong>Model</strong>. We first define the model distribution that takes \(\tx\) and predicts each token <strong>independently</strong> given the contextual embedding:</p>\[p(x_i \mid \tx) = \softmax(E \phi(\tx)_i).\]<p><strong>Masking function</strong>. We define a (stochastic) noising function \(A(\tx \mid \x)\) that:</p>\[\underbrace{\x}_\text{original} \stackrel{A}{\Rightarrow} \underbrace{\tx}_\text{noised}.\]<p>Here’s how \(A\) is defined:</p><ol><li>Let \(I \subset \{1, \dots, L\}\) be a random 15% of the tokens positions.<li>For each \(i \in I\):<ul><li>With probability 0.8, set \(\tilde x_i \leftarrow \MASK\).<li>With probability 0.1, set \(\tilde x_i \leftarrow x_i\).<li>With probability 0.1, set \(\tilde x_i \leftarrow \text{random word from } \sV\).</ul></ol><p><strong>Reducing distribution shift</strong>. If we were to always replace chosen tokens in \(I\) with \(\MASK\), then:</p><ul><li>During training, every input BERT would only see sequences with a \(\MASK\).<li>At test time, we would feed in sentences with no \(\MASK\), resulting in a distribution shift. The heuristic fix is to replace with real words 20% of the time.</ul><p><strong>Next sentence prediction</strong>. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not.</p><p>\([\CLS, \nl{the}, \nl{mouse}, \nl{ate}, \nl{the}, \nl{cheese}, \SEP, \nl{it}, \nl{was}, \nl{full}] \Rightarrow 1\).</p><p>\([\CLS, \nl{the}, \nl{mouse}, \nl{ate}, \nl{the}, \nl{cheese}, \SEP, \nl{hello}, \nl{world}] \Rightarrow 0\).</p><p>We will use the embedding of the \(\CLS\) token to make this binary classification decision.</p><p><strong>Dataset</strong>. Let \(\sD\) be a set of examples \((\x, c)\) constructed as follows:</p><ul><li>Let \(A\) be a sentence from the corpus.<li>With probability 0.5, let \(B\) be the next sentence.<li>With probability 0.5, let \(B\) be a random sentence from the corpus.<li>Let \(\x = [\CLS, A, \SEP, B]\).<li>Let \(c\) denote whether \(B\) is the next sentence or not.</ul><p><strong>Objective</strong>. Then the BERT objective is:</p>\[\sO(\theta) = \sum_{(\x,c) \in \sD} \underbrace{\E_{I, \tx \sim A(\cdot \mid \x, I)}\left[\sum_{i \in I} -\log p_\theta(\tilde x_i \mid \x)\right]}_\text{masked language modeling} + \underbrace{-\log p(c \mid \phi(\x)_1)}_\text{next sentence prediction}.\]<p>We will talk about training later, but a few quick notes about BERT:</p><ul><li>BERT (along with <a href="https://arxiv.org/pdf/1802.05365.pdf">ELMo</a> and <a href="https://arxiv.org/pdf/1801.06146.pdf">ULMFiT</a>) showed that one uniform architecture (Transformer) could be used for many multiple classification tasks.<li>BERT really transformed the NLP community into a pre-training + fine-tuning mindset.<li>BERT showed the importance of having deeply bidirectional contextual embeddings, although it’s possible that model size and fine-tuning strategies make up for it (<a href="https://arxiv.org/pdf/2103.10385.pdf">p-tuning</a>).</ul><p><a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa</a> makes the following changes to BERT:</p><ul><li>Removed the next sentence prediction objective (found it didn’t help).<li>Trained on more data (16GB text \(\rightarrow\) 160GB text).<li>Trained for longer. RoBERTa improved accuracy significantly over BERT on various benchmarks (e.g., on SQuAD 81.8 to 89.4).</ul><h3 id="encoder-decoder-models"> <a href="#encoder-decoder-models" class="anchor-heading" aria-labelledby="encoder-decoder-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Encoder-decoder models</h3><p>Example task (table-to-text generation):</p>\[[\nl{name}, \nl{:}, \nl{Clowns}, \nl{|}, \nl{eatType}, \nl{:}, \nl{coffee}, \nl{shop}] \Rightarrow [\nl{Clowns}, \nl{is}, \nl{a}, \nl{coffee}, \nl{shop}].\]<p>Recall that encoder-decoder models (e.g., BART, T5):</p><ul><li>Encode the input bidirectionally like BERT.<li>Decode the output autoregressively like GPT-2.</ul><p><strong>BART (Bidirectional Auto-Regressive Transformers)</strong>. BART (<a href="https://arxiv.org/pdf/1910.13461.pdf">Lewis et al. 2019</a>) is a Transformer-based encoder-decoder model.</p><ul><li>Same encoder architecture as RoBERTa (12 layers, hidden dimension 1024).<li>Trained on same data as RoBERTa (160GB text).</ul><p>BART considers the following transformations \(A(\tx \mid \x)\): <img src="../images/bart-transformations.png" alt="BART transformations" /> Based on BERT-scaled experiments, they decided on the following transformations for the final model:</p><ul><li>Mask 30% of tokens in a document<li>Permute all sentences</ul><p>They demonstrated strong results on both classification and generation tasks using fine-tuning.</p><p><strong>T5 (Text-to-Text Transfer Transformer)</strong>.</p><p>T5 (<a href="https://arxiv.org/pdf/1910.10683.pdf">Raffel et al., 2020</a>) is another Transformer-based encoder-decoder model.</p><p>Tasks:</p><ul><li>Given a span of text, split at random point into input and output:</ul>\[[\nl{the}, \nl{mouse}] \Rightarrow [\nl{ate}, \nl{the}, \nl{cheese}].\]<p>This paper experimented with many different unsupervised objectives: <img src="../images/t5-unsupervised-table.png" alt="T5 unsupervised tasks" /> and found that the “i.i.d. noise, replace spans” worked well (though many objectives were similar).</p><p>They also cast all classical NLP tasks in a uniform framework as “text-to-text” tasks: <img src="../images/t5-supervised.png" alt="T5 supervised tasks" /> Note the difference in approach to classification tasks:</p><ul><li>BERT used the embedding of the \(\CLS\) token to predict.<li>T5, GPT-2, GPT-3, etc. (models that can generate) cast the classification tasks in a <strong>natural</strong> language space.</ul><p>Notes:</p><ul><li>The paper does a thorough <strong>study</strong> of many aspects of the entire pipeline (dataset, model size, training objective, etc.).<li>Based on the insights, they trained a <strong>11B parameter</strong> model.</ul><h2 id="optimization-algorithms"> <a href="#optimization-algorithms" class="anchor-heading" aria-labelledby="optimization-algorithms"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Optimization algorithms</h2><p>Now we turn our attention to how to optimize the objective. For simplicity, let’s take autogressive language modeling:</p>\[\sO(\theta) = \sum_{\x \in \sD} -\log p_\theta(\x).\]<p><strong>Stochastic gradient descent (SGD)</strong>. A first cut is just to do stochastic gradient descent with mini-batches:</p><ul><li>Initialize parameters \(\theta_0\).<li>Repeat:<ul><li>Sample a mini-batch \(B_t \subset \sD\).<li>Perform a gradient step:</ul></ul>\[\theta_t \leftarrow \theta_{t-1} - \eta \frac{1}{|B_t|} \sum_{\x \in B_t} \nabla_\theta (-\log p_\theta(\x)).\]<p>The key concerns in optimization are:</p><ol><li>We want \(\theta\) to converge <strong>quickly</strong> to a good solution.<li>We want the optimization to be numerically <strong>stable</strong>.<li>We want to be <strong>memory efficient</strong> (especially for large models). These are often at odds with each other (e.g., fast convergence and cutting down on memory by low-precision produces less stable training).</ol><p>There are several levels that we can approach optimization:</p><ol><li>Classic optimization: second-order methods, constrained optimization, etc.<li>Machine learning: stochastic methods, implicit regularization + early stopping<li>Deep learning: initialization, normalization (changes to the model architecture)<li>Large language models: stability issues, weird learning rates While some of the intuitions (e.g., second-order methods) are still useful, there are many other unique challenges that need to be overcome for large language model training to work. Unfortunately, much of this is fairly ad-hoc and poorly understood.</ol><p><strong>ADAM (adaptive moment estimation)</strong>. <a href="https://arxiv.org/pdf/1412.6980.pdf">ADAM</a> incorporates two ideas:</p><ul><li>Use <strong>momentum</strong> (keep on moving in the same direction).<li><p>Have an <strong>adaptive</strong> (different) step size for each dimension of \(\theta\) (inspiration from second-order methods).</p><li>Initialize parameters \(\theta_0\).<li>Initialize moments \(m_0, v_0 \leftarrow 0\).<li>Repeat:<ul><li>Sample a mini-batch \(B_t \subset \sD\).<li>Update parameters as follows.</ul></ul><p><strong>Updating parameters</strong>.</p><ul><li>Compute gradient:</ul>\[g_t \leftarrow \frac{1}{|B_t|} \sum_{\x \in B_t} \nabla_\theta (-\log p_\theta(\x)).\]<ul><li>Update first- and second-order moments:</ul>\[m_t \leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t\] \[v_t \leftarrow \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]<ul><li>Do bias correction:</ul>\[\hat m_t \leftarrow m_t / (1 - \beta_1^t)\] \[\hat v_t \leftarrow v_t / (1 - \beta_2^t)\]<ul><li>Update parameters:</ul>\[\theta_t \leftarrow \theta_{t-1} - \eta \, \hat m_t / (\sqrt{\hat v_t} + \epsilon).\]<p><strong>Memory</strong>. Using Adam increases the amount of storage from \(2(\text{num-params})\) (from \(\theta_t,g_t\)) to \(4(\text{num-params})\) (from \(\theta_t,g_t,m_t,v_t\)).</p><p>AdaFactor (<a href="https://arxiv.org/pdf/1804.04235.pdf">Shazeer &amp; Stern, 2018</a>) was proposed as a way to reduce this memory footprint.</p><ul><li>Instead of storing the moments (\(m_t,v_t\)) of a \(O(m \times n)\) matrix, store row and column sums (\(O(m + n)\) memory) and reconstruct the matrix.<li>Remove momentum.<li>It was used to train T5.<li>It can be difficult to get AdaFactor to train (see <a href="https://twitter.com/_arohan_/status/1468673364889726985?s=20&amp;t=i7E0NN5ytysukMGVWG7lfQ">Twitter thread</a> and <a href="https://blog.ceshine.net/post/adafactor/">blog post</a>).</ul><p>Mixed-precision training is another method for reducing memory (<a href="https://arxiv.org/pdf/1710.03740.pdf">Narang et al., 2018</a>).</p><ul><li>Default: FP32 (32-bit floating point).<li>Option: FP16 (16-bit floating point), but the problem is that any value less than \(2^{-24}\) becomes 0.<li>Solution: store master weights in FP32 and do everything else in FP16.<li>Loss scaling: scale up loss to avoid gradients with small magnitudes.<li>Result: Halves the memory usage.</ul><p><img src="../images/mixed-precision-training.png" alt="Mixed-precision training" /></p><p><strong>Learning rates</strong>.</p><ul><li>Normally, the learning rate <strong>decreases</strong> over time.<li>For Transformers, we actually need to <strong>increase</strong> the learning rate (warmup).<li><a href="https://www.cs.toronto.edu/~mvolkovs/ICML2020_tfixup.pdf">Huang et al., 2020</a> show that a potential reason for this is to prevent vanishing gradients from layer normalization leads to instability in Adam optimizer.</ul><p><strong>Initialization</strong>.</p><ul><li>Given a matrix \(W \in \R^{m \times n}\), the standard initialization (xavier initialization) is \(W_{ij} \sim \sN(0, 1/n)\), where \(n\) is the fan-in.<li>GPT-2 and GPT-3 scale the weights by an additional \(1/\sqrt{N}\), where \(N\) is the number of residual layers.<li>T5 scales the attention matrices by an additional \(1/\sqrt{d}\) (<a href="https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/attention.py#L459">code</a>).</ul><p>For GPT-3:</p><ul><li>Adam parameters: \(\beta_1 = 0.9\), \(\beta_2 = 0.95\), \(\epsilon = 10^{-8}\).<li>Batch size: 3.2 million tokens (~1500 sequences)<li>Use gradient clipping (\(g_t \leftarrow g_t / \min(1, \|g\|_2)\)).<li>Linear learning rate warmup (over first 375 million tokens).<li><a href="https://arxiv.org/pdf/1608.03983v5.pdf">Cosine learning rate</a> that goes down to 10% of value.<li>Gradually increase the batch size.<li>Weight decay 0.1.</ul><h2 id="further-reading"> <a href="#further-reading" class="anchor-heading" aria-labelledby="further-reading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Further reading</h2><ul><li><a href="https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html#mixed-precision-training">Mixed precision training</a><li><a href="https://arxiv.org/pdf/1711.05101.pdf">Fixing Weight Decay Regularization in Adam</a>. <em>I. Loshchilov, F. Hutter</em>. 2017. Introduces <strong>AdamW</strong>.<li><a href="https://arxiv.org/pdf/2003.10555.pdf">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a>. <em>Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning</em>. ICLR 2020.<li><a href="https://arxiv.org/pdf/2006.03654.pdf">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a>. <em>Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen</em>. ICLR 2020.</ul><hr><footer> <!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Untitled</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css"><link rel="stylesheet" href="assets/css/style.css"><body><div class="footer-clean"><footer><div class="container"><div class="row justify-content-center"><p class="copyright">Made and Compiled by <a href="https://www.linkedin.com/in/joyanta0/">Joyanta.</a></p><p class="copyright">Department of Computer Science and Engineering, School of Data and Sciences BRAC University © 2023</p></div></div></div></footer></div><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.bundle.min.js"></script></footer></div></div><div class="search-overlay"></div></div>
